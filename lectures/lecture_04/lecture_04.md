---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.17.3
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
# Лекция 4: Подготовка данных (продолжение)

Машинное обучение и анализ данных

МГТУ им. Н.Э. Баумана

Красников Александр Сергеевич

2024-2025
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Обработка временных признаков


**Время — это не просто метка, это мощный источник информации о трендах и циклических паттернах.**
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Что такое Временной Признак?**

* **Определение:** Временной признак — это момент наблюдения (дата, дата+время) для каждого объекта в датасете.
*   **Пример:** В задаче прогнозирования спроса на велопрокат вектор признаков включает погоду, температуру и **дату наблюдения**.
*   **Проблема:** "Сырая" дата (например, `2023-10-27`) неинформативна для большинства алгоритмов ML. Ее нужно преобразовать.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как Кодировать Временной Признак?**

*   **Рекомендация:** Заменить исходную дату на числовое значение, отражающее прошедшее время.
*   **Метод 1 (Только дата):** `Число дней с начала наблюдений`. Находим самую раннюю дату в датасете и вычисляем разницу в днях для каждой записи.
*   **Метод 2 (Дата и время):** `Число секунд с начала измерений`. Более высокая гранулярность для задач, где важен точный момент времени.
*   **Зачем?** Это создает непрерывную числовую шкалу, которую алгоритмы могут интерпретировать как прогресс времени.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Временной Тренд**

*   **Определение:** Тренд — это долгосрочное, устойчивое направление изменения целевой переменной (рост или падение).
*   **Как учитывается?** Преобразованный признак "Число дней/секунд" **автоматически** позволяет модели уловить тренд.
*   **Пример (Велопрокат):** Спрос растет из года в год из-за роста популярности экологичного транспорта. Модель, видя, что "число дней" увеличивается, а вместе с ним и спрос, усвоит этот тренд.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Сезонность: Понятие и Важность**

*   **Определение:** Сезонность — это **периодическая** зависимость целевой величины от времени. Это повторяющиеся паттерны.
*   **Пример (Велопрокат):** Спрос зависит от дня недели (выше в выходные) и времени года (выше летом, ниже зимой). Это классическая сезонность.
*   **Ключевая мысль:** Тренд и сезонность — это разные вещи. Тренд — долгосрочное направление, сезонность — краткосрочные циклы.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как Учесть Сезонность? (Часть 1)**

**Основной метод:** Создать новые **категориальные** признаки из даты/времени.
* **Обязательные:**
  *   `Месяц` (1-12): для учета годовой сезонности.
  *   `День недели` (1-7 или Пн-Вс): для учета недельной сезонности.
*   **Дополнительно (если есть время):** `Номер часа` (0-23) для учета внутридневной сезонности (например, пик заказов такси утром и вечером).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как Учесть Сезонность? (Часть 2)**

* **Праздничные дни:** Создать бинарный признак `is_holiday` (0/1). Поведение в праздники часто сильно отличается от обычных дней.
* **Сезонные агрегаты (лаги):** Использовать значения целевой переменной с прошлых периодов.
  * *Пример:* Для прогноза спроса на велосипеды *сегодня* использовать спрос *неделю назад* или *год назад*.
  * *Продвинуто:* Использовать не только локальные лаги (спрос в Зеленограде), но и агрегированные (спрос по всей Москве или России).
<!-- #endregion -->

```python editable=true slideshow={"slide_type": "slide"}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

# Создаем данные
np.random.seed(42)
start_date = datetime(2024, 1, 1)
n_days = 365

# Генерируем даты
dates = [start_date + timedelta(days=x) for x in range(n_days)]

# Создаем временной признак (дни с начала наблюдений)
days_from_start = np.arange(n_days)

# Создаем тренд (линейный рост)
trend = 0.5 * days_from_start

# Создаем сезонность (недельная + годовая)
weekly_seasonality = 20 * np.sin(2 * np.pi * days_from_start / 7)
yearly_seasonality = 30 * np.sin(2 * np.pi * days_from_start / 365)

# Создаем шум
noise = np.random.normal(0, 10, n_days)

# Целевая переменная (например, количество арендованных велосипедов)
y = 100 + trend + weekly_seasonality + yearly_seasonality + noise

# Создаем DataFrame
df = pd.DataFrame({
    'date': dates,
    'days_from_start': days_from_start,
    'value': y,
    'trend': 100 + trend
})

# Создаем график
plt.figure(figsize=(12, 8))

# Основной график данных
plt.plot(df['date'], df['value'], 
         label='Фактические данные (с сезонностью и шумом)', 
         linewidth=1, alpha=0.7, color='blue')

# Линия тренда
plt.plot(df['date'], df['trend'], 
         label='Тренд (линейный рост)', 
         linewidth=3, color='red', linestyle='--')

# Настройки графика
plt.title('Временной ряд с трендом и сезонностью\n(Пример: аренда велосипедов)', 
          fontsize=14, fontweight='bold', pad=20)
plt.xlabel('Дата', fontsize=12)
plt.ylabel('Количество аренд', fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# Добавляем аннотации
plt.annotate('Восходящий тренд', 
             xy=(df['date'].iloc[180], df['trend'].iloc[180]), 
             xytext=(df['date'].iloc[120], df['trend'].iloc[120] + 50),
             arrowprops=dict(arrowstyle='->', color='red', lw=1.5),
             fontsize=10, color='red')

plt.annotate('Сезонные колебания\n(недельные/годовые)', 
             xy=(df['date'].iloc[250], df['value'].iloc[250]), 
             xytext=(df['date'].iloc[200], df['value'].iloc[200] - 80),
             arrowprops=dict(arrowstyle='->', color='blue', lw=1.5),
             fontsize=10, color='blue')

plt.tight_layout()
plt.show()

# Дополнительный график: только тренд vs данные без тренда
plt.figure(figsize=(12, 6))

# Данные без тренда (только сезонность + шум)
detrended_data = df['value'] - df['trend'] + 100

plt.plot(df['date'], df['value'], label='Исходные данные', alpha=0.7)
plt.plot(df['date'], detrended_data, label='Данные без тренда', alpha=0.7)
plt.plot(df['date'], df['trend'], label='Тренд', linewidth=2, color='red')

plt.title('Разложение временного ряда: данные = тренд + сезонность + шум', 
          fontsize=14, fontweight='bold')
plt.xlabel('Дата')
plt.ylabel('Значение')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Выводим статистику
print("Статистика временного ряда:")
print(f"Начальное значение: {df['value'].iloc[0]:.1f}")
print(f"Конечное значение: {df['value'].iloc[-1]:.1f}")
print(f"Общий рост за период: {df['value'].iloc[-1] - df['value'].iloc[0]:.1f}")
print(f"Среднесуточный рост: {(df['value'].iloc[-1] - df['value'].iloc[0]) / n_days:.2f}")
```

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Влияние обработки времени на качество модели**

*   **Без учета:** Модель не видит тренд и сезонность, прогнозы хаотичны и неточны.
*   **С учетом:** Модель, используя преобразованный временной признак и категориальные признаки (месяц, день недели), точно отслеживает как долгосрочный тренд, так и сезонные колебания.
*   **Вывод:** Правильная обработка временного признака — ключ к качественному прогнозированию временных рядов.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Итоги и рекомендации**

1.  **Не используйте "сырую" дату.** Всегда преобразуйте ее в числовой формат (дни/секунды с начала).
2.  **Всегда извлекайте сезонные признаки.** Минимум — `месяц` и `день недели`.
3.  **Учитывайте контекст.** Добавляйте `праздники`, `часы` и `лагированные значения`, если это уместно для вашей задачи.
4.  **Визуализируйте.** Постройте графики до и после обработки, чтобы убедиться, что вы захватили тренд и сезонность.

<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Обработка категориальных признаков

**От One-Hot до Target Encoding. Как заставить модель понять "текст".**
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Что такое категориальные признаки?**

*   **Определение:** Признаки, которые могут принимать одно из конечного набора дискретных значений (категорий). Например, город рождения, марка машины, тип продукта.
*   **Частный случай:** **Бинарные признаки** — принимают только два значения (0/1), например, "Есть кредит" или "Пол".
*   **Проблема:** Большинство алгоритмов ML работают **только с числами**. Нельзя просто подставить "Москва" или "Врач" в модель.
*   **Решение:** **Кодирование** — преобразование категорий в числовые векторы.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### Примеры категориальных признаков


| Тип признака | Описание | Примеры значений | Количество категорий | Особенности |
|-------------|----------|------------------|---------------------|-------------|
| **Город** | Географическая принадлежность | Москва, СПб, Казань, Новосибирск, Екатеринбург | 5+ | Порядок не важен, можно использовать one-hot encoding |
| **Цвет** | Визуальная характеристика | Красный, Синий, Зеленый, Желтый, Черный | 5+ | Номинальная переменная, нет естественного порядка |
| **Профессия** | Род занятий | Врач, Учитель, Инженер, Программист, Менеджер | 10+ | Может иметь иерархию (специализация внутри профессии) |
| **Тип жилья** | Вид недвижимости | Квартира, Дом, Таунхаус, Апартаменты | 4 | Категории взаимоисключающие |
| **Образование** | Уровень образования | Среднее, Среднее специальное, Высшее, Бакалавр, Магистр | 5 | Может быть порядковым признаком |
| **Семейное положение** | Социальный статус | Холост/Не замужем, Женат/Замужем, Разведен(а), Вдовец/Вдова | 4 | Категории не упорядочены |
| **Бренд** | Производитель товара | Apple, Samsung, Xiaomi, Huawei, Sony | 5+ | Может влиять на цену и восприятие качества |
| **Сезон** | Время года | Весна, Лето, Осень, Зима | 4 | Циклический признак, имеет порядок |
| **День недели** | День недели | Понедельник, Вторник, Среда, Четверг, Пятница, Суббота, Воскресенье | 7 | Порядковый и циклический признак |
| **Тип операции** | Вид банковской операции | Покупка, Перевод, Снятие наличных, Пополнение, Оплата услуг | 5 | Категории взаимоисключающие |
| **Статус заказа** | Состояние заказа | Оформлен, Оплачен, В обработке, Отправлен, Доставлен, Отменен | 6 | Может быть порядковым признаком |
| **Оценка качества** | Субъективная оценка | Плохо, Удовлетворительно, Хорошо, Отлично, Превосходно | 5 | Порядковый признак (ординальный) |
| **Группа крови** | Медицинский параметр | A, B, AB, O | 4 | Номинальная переменная |
| **Операционная система** | Тип ОС | Windows, macOS, Linux, Android, iOS | 5 | Категории не упорядочены |
| **Метод оплаты** | Способ расчета | Наличные, Карта, Перевод, Электронный кошелек | 4 | Может влиять на поведение клиента |

<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Порядковое кодирование (Ordinal Encoding)**

Каждой категории присваивается произвольный целочисленный номер (0, 1, 2, 3...).

| Профессия | Код |
|-----------|----------------|
| **Программист** | 0 |
| **Художник** | 1 |
| **Дизайнер** | 2 |
| **Системный администратор** | 3 |



* **Почему НЕ рекомендуется:** Алгоритм интерпретирует эти числа как порядковую шкалу. Он "подумает", что:
  *   `Сис.Админ (1)` "больше" или "лучше", чем `Программист (0)`.
  *   `Программист (0)` ближе к `Художнику (1)`, чем к `Сис.Админу (3)`.
* **Вывод:** Использовать **только** если категории имеют *реальный, естественный порядок* (например, "Низкий", "Средний", "Высокий").
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Кодирование Частотами (Frequency Encoding)**

Каждая категория заменяется на частоту ее появления в датасете (например, 3/7 = ~0.43).

 Профессия               | Кол-во | Доля | Процент |
| :---------------------- | :----- | :--- | :------ |
| Программист             | 2      | 2/7  | 28.57%  |
| Художник                | 1      | 1/7  | 14.29%  |
| Дизайнер                | 1      | 1/7  | 14.29%  |
| Системный администратор | 3      | 3/7  | 42.86%  |
| **ИТОГО**               | **7**  | 7/7  | 100.00% |

* **Плюсы:**
  * Очень компактно (одно число на категорию).
  *   Может быть полезно, если частота категории коррелирует с целевой переменной (например, популярные товары чаще покупают).
* **Минусы:**
  * Разные категории с одинаковой частотой получают одинаковый код (Дизайнер = Художник в примере).
  *   Не передает семантическую близость категорий.
* **Когда использовать:** Когда важна именно частота, а не смысл категории.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **One-Hot Кодирование (The Gold Standard)**

Для каждой уникальной категории создается новый бинарный признак (0 или 1). Если объект принадлежит категории — 1, иначе — 0.

**Исходные данные:**
 ID | Профессия |
|----|-----------|
| 1 | Программист |
| 2 | Системный администратор |
| 3 | Дизайнер |
| 4 | Художник |
| 5 | Программист |
| 6 | Системный администратор |
| 7 | Системный администратор |

**После One-Hot кодирования:**

| ID | Программист | Системный_администратор | Дизайнер | Художник |
|----|-------------|------------------------|----------|----------|
| 1 | **1** | 0 | 0 | 0 |
| 2 | 0 | **1** | 0 | 0 |
| 3 | 0 | 0 | **1** | 0 |
| 4 | 0 | 0 | 0 | **1** |
| 5 | **1** | 0 | 0 | 0 |
| 6 | 0 | **1** | 0 | 0 |
| 7 | 0 | **1** | 0 | 0 |



* **Плюсы:**
  * **Идеально:** Все категории равнозначны. Расстояние между любыми двумя категориями одинаково.
  * **Интуитивно понятно** для моделей.
* **Минусы:**
  * **"Проклятие размерности":** Если категорий много (например, 1000 городов), создается 1000 новых признаков. Это замедляет обучение и может привести к переобучению.
* **Как бороться:**
  * Объединять редкие категории в одну ("Другие города").
  * Использовать регуляризацию модели (L1/L2).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Эмбеддинги: Компактная альтернатива One-Hot**

* **Проблема One-Hot:** Слишком длинные векторы для большого числа категорий.
* **Решение — Эмбеддинги:** Представление категории в виде компактного вектора вещественных чисел (например, длиной 5-50, а не 1000).
* **Как создаются:**
  * **Случайно:** Но с контролем расстояний.
  * **Семантически:** Близкие по смыслу категории (Программист и Сис.Админ) имеют близкие векторы.
* **Где применяются:** Очень популярны в NLP (Word2Vec) и для категорий с высокой кардинальностью (например, ID пользователей).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Кодирование средним / Target Encoding (Самый мощный)**

* **Метод:** Каждая категория заменяется на **среднее значение целевой переменной** для всех объектов этой категории.
  * *Регрессия (зарплата):* "Программист" -> средняя зарплата всех программистов.
  * *Классификация (кредит):* "Программист" -> доля программистов, вернувших кредит.
* **Плюсы:**
  * **Максимально информативно:** Прямо встраивает информацию о цели в признак.
  * **Очень компактно:** Один числовой признак на категорию.
* **Главный минус: Риск переобучения (Data Leakage)!**
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как избежать переобучения в Target Encoding?**

* **Проблема:** Если категория встречается редко (например, "Художник" — 1 человек с зарплатой 150), модель "запомнит" это значение и будет переобучаться.
* **Решения:**
  1. **Leave-One-Out (LOO):** Для кодирования объекта `i` используется среднее значение **по всем другим объектам** той же категории, *кроме самого объекта `i`*. Это самый надежный метод.
  2. **Добавление шума/сглаживания:** `Код = (Среднее * Кол-во + Глобальное_Среднее * m) / (Кол-во + m)`, где `m` — параметр сглаживания.
  3. **Разделение выборки:** Кодировать на одной части данных, обучать модель — на другой.
* **Вывод:** Target Encoding — очень мощный инструмент, но требует аккуратной реализации!
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Кодирование циклических переменных**

* **Проблема:** Обычные методы кодирования (например, час 23=23, час 0=0) не учитывают, что 23:00 и 01:00 — соседние моменты времени.
* **Решение — Циклическое Кодирование:** Используем пару признаков на основе синуса и косинуса.
  * `Час_sin = sin(2π * Час / 24)`
  * `Час_cos = cos(2π * Час / 24)`
* **Почему работает:** Синус и косинус — периодические функции. Значения для 23 и 0 часов будут очень близки в этом новом пространстве.
* **Когда использовать:** Для времени суток, дней недели, месяцев, дней года — везде, где есть цикличность.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как выбрать метод кодирования?**

*   **Нет "лучшего" метода.** Выбор зависит от данных, модели и задачи.
*   **One-Hot:** Безопасный выбор по умолчанию для небольшого числа категорий (< 10-20).
*   **Target Encoding:** Лучший выбор для категорий с высокой кардинальностью (город, ID пользователя) — **но только с LOO или сглаживанием!**
*   **Frequency Encoding:** Если частота сама по себе информативна.
*   **Ordinal Encoding:** **Только** для упорядоченных категорий.
*   **Циклическое:** Для часов, дней, месяцев.
*   **Экспериментируйте!** Часто выгодно создавать несколько версий признака разными методами.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Нормализация признаков

Почему масштаб имеет значение и как привести все признаки к общему знаменателю.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Проблема разного масштаба**

*   **Факт:** В реальных датасетах признаки имеют совершенно разные диапазоны значений. Один — от -0.01 до 0.01, другой — от 0 до 100 000.
*   **Проблема:** Для **большинства** алгоритмов ML (особенно метрических, как 
KNN, SVM, градиентный бустинг) признак с б**о**льшим разбросом значений будет 
доминировать в расчетах, "забивая" признаки с меньшим масштабом. После 
масштабирования одного признака — ближайший объект меняется, и меняется 
прогноз! Это нечестно и неправильно.
*   **Вывод:** Чтобы все признаки влияли на модель **поровну**, их нужно привести к одному масштабу — **нормализовать**.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Основные методы нормализации**

*   **Заголовок:** Три Главных Способа Нормализации

    *   **1. Стандартизация (Standard Scaler):**
        *   **Формула:** `(X - среднее) / стандартное_отклонение`
        *   **Результат:** Признак имеет **среднее = 0** и **стандартное отклонение = 1**.
        *   **Когда использовать:** Самый популярный и универсальный метод. Подходит для большинства случаев, особенно если данные распределены нормально.
    *   **2. Диапазонное шкалирование (Min-Max Scaler):**
        *   **Формула:** `(X - min) / (max - min)`
        *   **Результат:** Все значения признака лежат в диапазоне **[0, 1]**.
        *   **Плюс:** Сохраняет **разреженность** данных (нули остаются нулями). Идеально для текстовых данных или One-Hot кодирования.
    *   **3. Нормализация средним (Mean Normalization):**
        *   **Формула:** `(X - среднее) / (max - min)`
        *   **Результат:** Признак имеет **среднее = 0** и **диапазон = 1**.
        *   **Когда использовать:** Реже, но полезно, когда нужно и центрировать данные, и контролировать диапазон.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как это работает на практике?**

*   **Заголовок:** Пример: Стандартизация До и После

*   Представим датасет: `Рост: [160, 175, 190]`, `Вес: [50, 70, 90]`.
*   **До:** Рост ~175±15, Вес ~70±20. Вес имеет больший разброс.
*   **После Стандартизации:**
  *   `Рост_std: [-1.0, 0.0, 1.0]`
  *   `Вес_std: [-1.0, 0.0, 1.0]`
*   **Итог:** Теперь оба признака имеют одинаковый масштаб и будут влиять на модель одинаково. Алгоритм не будет "думать", что вес важнее роста только из-за единиц измерения.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Враг нормализации — выбросы**

* **Проблема:** Классические методы (среднее, стандартное отклонение, min/max) **крайне чувствительны** к выбросам.
* **Пример:** Если в данных по зарплатам есть один CEO с зарплатой $1 000 000, а остальные — $50 000, то:
  * `среднее` будет сильно смещено вверх.
  * `max` будет равен 1 000 000.
  * В результате нормализации все "нормальные" зарплаты будут сжаты в очень маленький диапазон, почти в ноль.
* **Вывод:** Перед нормализацией **обязательно** нужно обработать выбросы!
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Робастные (устойчивые) методы**

Заменить чувствительные статистики на **робастные** (устойчивые к выбросам).

*   **Вместо Среднего -> Медиана (Median):** Значение, которое делит выборку пополам. 50% значений меньше, 50% — больше. Не зависит от экстремальных значений.
*   **Вместо Стандартного Отклонения -> MAD (Median Absolute Deviation):** Медиана абсолютных отклонений от медианы. Гораздо устойчивее.
*   **Вместо Min/Max -> Перцентили (например, 1% и 99%):** Игнорируем самые крайние 1% значений с каждой стороны.
*   **Формула робастной стандартизации:** `(X - медиана) / MAD`
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Что такое медиана и перцентиль?**

* **Медиана:** Это **не** среднее арифметическое. Это середина упорядоченного списка. Если у вас зарплаты [30k, 40k, 50k, 60k, 1M], медиана = 50k, а среднее = 236k.
* **Перцентиль (Percentile):** Значение, ниже которого лежит определенный процент данных.
  * **50-й перцентиль = Медиана.**
  * **1-й перцентиль:** 1% данных меньше этого значения.
  * **99-й перцентиль:** 99% данных меньше этого значения (т.е. это почти максимум, но без учета самых крайних выбросов).
* **Применение:** Используем 1-й и 99-й перцентили вместо min/max для робастного Min-Max скалирования.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Итоги и рекомендации**

1.  **Всегда нормализуйте!** Если алгоритм чувствителен к масштабу (а большинство — чувствительны), нормализация — must-have.
2.  **Стандартизация — ваш друг.** Начинайте с нее, если нет особых причин делать иначе.
3.  **Min-Max для разреженных данных.** Если у вас много нулей (текст, One-Hot), используйте Min-Max, чтобы их сохранить.
4.  **Бойтесь выбросов.** Всегда визуализируйте данные (boxplot, гистограмма) перед нормализацией.
5.  **Используйте робастные методы.** Если выбросы есть и их нельзя удалить — применяйте нормализацию на основе медианы и перцентилей.
6.  **Fit только на трейне!** Обучайте scaler (вычисляйте среднее, std, min, max) **только** на обучающей выборке. Затем применяйте `transform` и к train, и к test. Это предотвращает утечку данных (data leakage).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Генерация признаков

**Искусство создавать "волшебные" признаки для повышения точности моделей.**
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Зачем генерировать признаки?**

Многие алгоритмы (особенно линейные) могут моделировать только простые зависимости. Реальный мир сложнее!

*   **Пример:** Линейная модель не сможет точно предсказать цену участка, зная только его длину и ширину по отдельности. Но если добавить **площадь (длина * ширина)** — точность резко возрастет.
*   **Цель Feature Engineering:** Создать признаки, которые **точно соответствуют фрагментам реальной зависимости** в данных. Модель должна легко "собрать" из них итоговое решение.
*   **Итог:** Генерация признаков — это ключ к раскрытию потенциала ваших данных.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Основные подходы: Часть 1 — Интерпретируемые признаки**

* **Суть:** Использовать знание предметной области для создания осмысленных комбинаций или преобразований.
* **Примеры:**
  * **Произведение:** Площадь участка, Объем (длина*ширина*высота).
  * **Отношение:** Средний размер комнаты (Жилая площадь / Число комнат), Плотность населения = Население / Площадь.
  * **Сумма/Разность:** Общий доход семьи, Разница между макс. и мин. температурой за день.
* **Преимущество:** Полученные признаки легко интерпретировать и объяснить бизнесу.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Основные подходы: Часть 2 — Биннинг и Кластеризация**

*   **Биннинг (Дискретизация):** Разбиваем непрерывный признак на категории.
  *   *Пример:* Создаем 3 бинарных признака из возраста:
      *   `I{age < 18}` (ребенок)
      *   `I{18 <= age < 65}` (взрослый)
      *   `I{age >= 65}` (пенсионер)
  *   *Зачем?* Позволяет линейной модели обрабатывать разные группы по-разному.

*   **Кластеризация:** Группируем объекты по схожести и используем номер кластера как новый признак.
  *   *Пример:* Кластеризуем клиентов по поведению -> добавляем признак "Номер сегмента клиента".
  *   *Дополнительно:* Можно добавить расстояние до центра своего кластера (мера "типичности").
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Основные подходы: Часть 3 — Учет специфики алгоритмов**

* **Для решающих реревьев (Decision Trees):**
  * Деревья отлично создают сложные правила из простых признаков.
  * **Но:** Им сложно моделировать линейные комбинации. Поэтому **помогите им** — создайте признаки-суммы/разности/произведения вручную.
* **Для метрических алгоритмов (KNN, SVM):**
  * Эти алгоритмы смотрят на *расстояния* между объектами.
  * **Помогите им:** Создайте признаки, которые лучше отражают "похожесть". Это может быть и биннинг, и линейные комбинации.
* **Для всех алгоритмов:**
  * Можно добавить **расстояние до эталонных объектов** (например, до центров кластеров).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Основные подходы: Часть 4 — Ансамбли и Мета-признаки**

*   **Суть:** Прогнозы одной модели могут стать отличным признаком для другой, более сложной модели.
*   **Что это дает:** Финальная модель учится оптимально **агрегировать** (объединять) прогнозы базовых моделей. Это называется **ансамбль** или **композиция моделей**.
*   **Ключевое правило:** Чтобы избежать **переобучения**, базовые модели и финальная модель должны обучаться **на разных подвыборках данных** (например, в методах Stacking или Blending).
*   **Пример:** Прогноз линейной регрессии + прогноз решающего дерева -> входные данные для логистической регрессии.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Главная опасность: ложные зависимости**

** Больше Признаков ≠ Лучшая Модель!**

* **Главная угроза:** **Ложная зависимость (False Dependency)**. Это когда модель находит связь между сгенерированным признаком и целевой переменной, но эта связь — просто **случайное совпадение** в обучающих данных.
* **Пример:** Если сгенерировать 1000 случайных признаков, некоторые из них будут случайно сильно коррелировать с целевой переменной. Модель "запомнит" это, но на новых данных предсказания будут катастрофически плохими.
* **Как бороться:**
  1.  **Здравый смысл:** Генерируйте признаки, которые имеют логическое обоснование.
  2.  **Валидация:** Всегда проверяйте качество модели на отложенной (валидационной) выборке.
  3.  **Регуляризация:** Используйте методы (L1/L2), которые штрафуют модель за использование слишком большого числа признаков.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Итоги и рекомендации**

1.  **Feature Engineering — это must-have.** Не надейтесь, что сырые данные дадут вам хорошую модель.
2.  **Начинайте с домена.** Самые мощные признаки рождаются из понимания бизнес-логики и физики процесса.
3.  **Экспериментируйте.** Пробуйте разные комбинации, биннинг, кластеризацию.
4.  **Помогайте алгоритму.** Генерируйте признаки, которые компенсируют слабости выбранного алгоритма.
5.  **Бойтесь переобучения.** Всегда валидируйте результаты и используйте регуляризацию.
6.  **Качество важнее количества.** Один хороший, интерпретируемый признак лучше десяти случайных.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Сокращение числа признаков

**Почему "меньше" часто значит "лучше" в машинном обучении.**
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Проблема избыточных признаков**

* **Источники избыточности:**
  * Сбор "на всякий случай" даже слабоинформативных данных.
  * Активная генерация новых признаков (Feature Engineering).
* **Главные проблемы:**
  1. **Переобучение (Overfitting):** Модель с большим числом параметров (например, весов в линейной модели) начинает "запоминать" шум и случайные закономерности обучающей выборки, теряя способность обобщать на новые данные.
  2. **Вычислительная сложность:** Больше признаков = больше памяти для хранения, больше времени на обучение и предсказание.
  3. **Проклятие размерности:** В очень многомерном пространстве данные становятся разреженными, что затрудняет обучение многих алгоритмов.
* **Вывод:** Качество модели часто улучшается после отсева лишних, неинформативных или избыточных признаков.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Два основных подхода к сокращению**

* **1. Отбор признаков (Feature Selection):**
  * **Суть:** Выбираем **подмножество** из исходных признаков. Остальные **полностью отбрасываем**.
  * **Преимущество:** Сохраняется **интерпретируемость**. Мы знаем, какие именно исходные признаки использует модель.
  * **Пример:** Выбрать 10 самых важных столбцов из таблицы с 100 столбцами.
* **2. Снижение размерности (Dimensionality Reduction):**
  * **Суть:** Создаем **новые**, **искусственные** признаки как **преобразование** (часто линейную комбинацию) **всех** исходных признаков.
  * **Преимущество:** Максимально эффективно сжимает информацию, устраняя корреляции.
  * **Недостаток:** Потеря интерпретируемости. Новые признаки — это "черный ящик".
  * **Пример:** Преобразовать 100 исходных признаков в 10 новых компонент.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Методы отбора признаков (Feature Selection)**


* **Фильтрация (Filter Methods):** Оцениваем признаки **независимо от модели**, используя статистические тесты.
  * *Пример:* Выбрать признаки, наиболее сильно коррелирующие с целевой переменной (для регрессии — `r_regression`, для классификации — `f_classif`, `chi2`). Используется `SelectKBest` или `SelectPercentile` в sklearn.
* **Обертывание (Wrapper Methods):** Используем **саму модель** для оценки качества подмножества признаков.
  * *Пример:* Рекурсивное исключение признаков (`RFE`). Обучаем модель, отбрасываем самый "слабый" признак (с наименьшим весом или важностью), повторяем.
* **Встроенные методы (Embedded Methods):** Алгоритмы, которые **самостоятельно** отбирают признаки в процессе обучения.
  * *Пример:* Линейные модели с L1-регуляризацией (`Lasso`, `LogisticRegression(penalty='l1')`) обнуляют веса неинформативных признаков. Деревья (`RandomForest`, `XGBoost`) вычисляют важность признаков (`feature_importances_`).
* **Инструмент:** `SelectFromModel` в sklearn — универсальный способ отбора на основе важности признаков любой модели.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Метод главных компонент (PCA) — Король снижения размерности**

* **Суть:** PCA находит новые оси (главные компоненты) в данных, которые:
  1. **Ортогональны** друг другу (некоррелированы).
  2. **Максимизируют дисперсию** данных вдоль каждой оси. Первая компонента объясняет максимум вариации, вторая — оставшуюся часть и т.д.
* **Как работает:** Линейное преобразование, которое проецирует данные на подпространство меньшей размерности так, чтобы **сумма квадратов расстояний от исходных точек до их проекций была минимальной**.
* **Применение:**
  * Сокращение размерности перед обучением модели (ускорение, борьба с переобучением).
  * Визуализация многомерных данных (свести к 2D или 3D).
  * Устранение мультиколлинеарности (корреляции между признаками).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Как выбрать метод и количество признаков?**

1.  **Начните с отбора.** Если важна интерпретируемость — Feature Selection ваш выбор.
2.  **Используйте встроенные методы.** Если вы используете Lasso или деревья — они сами помогут отобрать признаки.
3.  **PCA — для "черного ящика".** Если интерпретируемость не критична, а нужна максимальная компрессия — используйте PCA.
4.  **Валидация — ключ!** Всегда оценивайте качество модели на **валидационной выборке** после сокращения признаков. Не оптимизируйтесь только под трейн!
5.  **Подбирайте количество признаков.** Для PCA — смотрите на долю объясненной дисперсии (например, оставьте 95%). Для `SelectKBest` — пробуйте разные `k`.
6.  **Помните о порядке.** Сокращение признаков — это шаг **предобработки**. Всегда делайте его **внутри** пайплайна, чтобы избежать утечки данных (data leakage).
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Итоги и рекомендации**

1.  **Избыток признаков вредит.** Он ведет к переобучению и замедляет работу.
2.  **Выбирайте подход:** `Feature Selection` для интерпретируемости, `Dimensionality Reduction` (PCA) для максимальной компрессии.
3.  **Используйте правильные инструменты:** `SelectKBest`, `RFE`, `SelectFromModel`, `PCA` из библиотеки `scikit-learn`.
4.  **Всегда валидируйте.** Качество модели на валидации — главный критерий успеха.
5.  **Стандартизируйте перед PCA.** Это обязательный шаг.
6.  **Делайте все в пайплайне.** Это предотвратит утечку данных и сделает ваш код надежным.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Преобразование целевой переменной

**Почему и как менять Y для лучшей модели.**
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Зачем преобразовывать целевую переменную?**

* **Причина 1: Обработка выбросов.** Аномальные значения Y (слишком большие или маленькие) могут быть вызваны ошибками измерения или редкими событиями. Они "сбивают с толку" модель, заставляя ее подстраиваться под шум. Удаление таких объектов повышает **стабильность** модели.
* **Причина 2: Линеаризация зависимости.** Многие модели (особенно линейные) предполагают **линейную** связь между X и Y. Но в реальности зависимость часто **нелинейна** (например, экспоненциальная). Преобразовав Y, мы можем сделать эту зависимость линейной, что упростит задачу для модели и повысит ее точность.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Основная идея: Трансформация и обратное преобразование**

Схема:
1.  `Исходные данные: (X, Y) -> Преобразование: Y_new = g(Y) -> Обучение модели на (X, Y_new)`
2.  `Прогноз модели: Y_pred_new -> Обратное преобразование: Y_pred = g^(-1)(Y_pred_new)`

*   **Шаг 1 (Обучение):** Мы применяем к целевой переменной Y **нелинейную функцию** `g()`. Получаем новую целевую переменную `g(Y)`. Обучаем модель на парах `(X, g(Y))`.
*   **Шаг 2 (Прогноз):** Когда модель делает прогноз `Ŷ_new` (в преобразованной шкале), мы применяем к нему **обратную функцию** `g^(-1)()`, чтобы вернуть прогноз в исходную шкалу: `Ŷ = g^(-1)(Ŷ_new)`.
*   **Ключевое:** Функция `g()` должна иметь обратную функцию `g^(-1)()`, чтобы мы могли интерпретировать итоговый прогноз.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Самый популярный пример: Логарифмирование**

* **Когда использовать:** Если целевая переменная Y принимает **только положительные значения** и имеет экспоненциальный рост (например, цены, доходы, спрос).
* **Функция:** $g(u) = ln(u)$ (натуральный логарифм).
* **Обратная функция:** $g^{-1}(u) = e^u$ (экспонента).
* **Зачем:** Логарифм "сплющивает" большие значения и "растягивает" маленькие, превращая экспоненциальную зависимость в линейную. Это также часто помогает сделать распределение Y более симметричным.
<!-- #endregion -->

```python editable=true slideshow={"slide_type": "slide"}
import numpy as np
import matplotlib.pyplot as plt

# Создаем данные
x = np.linspace(0, 4, 100)  # Значения X от 0 до 4
y_exp = np.exp(x)           # Экспоненциальная зависимость: Y = e^X
y_log = np.log(y_exp)       # Логарифмическая зависимость: ln(Y) = X (это просто x)

# Создаем фигуру с двумя подграфиками
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Левый график: Экспоненциальная кривая Y от X
axes[0].plot(x, y_exp, color='blue', linewidth=2, label=r'$Y = e^X$')
axes[0].set_title(r'Экспоненциальная зависимость: $Y = e^X$', fontsize=14)
axes[0].set_xlabel('X', fontsize=12)
axes[0].set_ylabel('Y', fontsize=12)
axes[0].grid(True, linestyle='--', alpha=0.7)
axes[0].legend(fontsize=12)

# Правый график: Линейная зависимость ln(Y) от X
axes[1].plot(x, y_log, color='red', linewidth=2, label=r'$\ln(Y) = X$')
axes[1].set_title(r'Линейная зависимость после логарифмирования: $\ln(Y) = X$', fontsize=14)
axes[1].set_xlabel('X', fontsize=12)
axes[1].set_ylabel(r'$\ln(Y)$', fontsize=12)
axes[1].grid(True, linestyle='--', alpha=0.7)
axes[1].legend(fontsize=12)

# Настраиваем отступы между графиками
plt.tight_layout()

# Показываем график
plt.show()
```

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Продвинутый метод: нормализация наспределения Y**

* **Идея:** Многие статистические методы и предположения моделей работают лучше, если целевая переменная имеет **нормальное (гауссово) распределение**.
* **Как:** Нужно найти такую функцию $g()$, которая преобразует распределение Y в стандартное нормальное распределение (среднее=0, дисперсия=1).
* **Формула:** $g(Y) = Φ^{-1}(F(Y))$
  * $F(Y)$ — кумулятивная функция распределения (CDF) исходной переменной Y.
  * $Φ^{-1}$ — обратная функция стандартного нормального распределения (квантильная функция).
* **Результат:** Преобразованная переменная $g(Y)$ будет иметь идеальное нормальное распределение.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Практика: как реализовать в Python (sklearn)**

*   **Инструмент:** В библиотеке `scikit-learn` есть специальный класс — `TransformedTargetRegressor`.
*   **Как работает:** Он оборачивает вашу основную модель (например, `LinearRegression`) и автоматически применяет преобразование к Y перед обучением и обратное преобразование к прогнозам.
*   **Пример кода:**
```python
from sklearn.linear_model import LinearRegression
from sklearn.compose import TransformedTargetRegressor
import numpy as np

# Создаем "обернутую" модель
model = TransformedTargetRegressor(
    regressor=LinearRegression(),  # Ваша базовая модель
    func=np.log,                   # Функция преобразования (ln)
    inverse_func=np.exp            # Обратная функция (exp)
)

# Обучаем модель — она сама преобразует Y
model.fit(X_train, y_train)

# Делаем прогноз — она сама делает обратное преобразование
y_pred = model.predict(X_test)
```

*   **Преимущество:** Это "честный" способ, который предотвращает утечку данных (data leakage), так как преобразование обучается только на тренировочных данных.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### **Итоги и рекомендации**

1.  **Не забывайте про Y.** Предобработка целевой переменной так же важна, как и предобработка признаков.
2.  **Удаляйте выбросы.** Если Y содержит аномальные значения — подумайте об их удалении для повышения стабильности.
3.  **Логарифмируйте положительные Y.** Это первый и самый простой шаг для борьбы с нелинейностью и асимметрией.
4.  **Стремитесь к нормальности.** Если ваша задача требует строгих статистических предположений — используйте `Φ^(-1)(F(Y))`.
5.  **Используйте TransformedTargetRegressor.** Это надежный и удобный инструмент в sklearn для автоматизации процесса.
6.  **Всегда делайте обратное преобразование.** Итоговый прогноз должен быть в исходной шкале для интерпретации!

<!-- #endregion -->
